
* There are sources and sinks (If they have to be connected over https , tcp it becomes very tedious and is tighly copuled)
* Apache Kafka can sit between all the sources and sinks
* Source need only be worried about pushing data to kafka and that's it
* Sink can only read data from kafka stream

Kafka Eco-system :

Source    ----------->  Producers 			 ---------> Kafka --------> Consumers  			-------> Target 
systems   ----------->	uses producer API    ---------> Kafka --------> users consumer API  -------> systems 
														|  |
														|  |
														|  |
														|  |
														|  |
													 Zoo Keeper

* Topics - A stream of data
        - Similar to tables in database without any restriction
		- Topic is identified by it's name
		- Topics are split into partitions
		- Inside a partition, a message is written in incremental order - offset
		- Each partition is ordered
* Offsets are only valid inside a partition
* Data is only kept for 2 weeks by default
* Once data is written to a partition, it cannot be changed - immutability --> cannot overwrite a message in Kafka
* Data is only written to Topic, partition is randomly assigned

==>Brokers - Kafka cluster is composed of multiple servers(Brokers)
* Broker is identified by ID
* Connection to one Broker(bootstrap broker) will connect to all the brokers in that cluster
* Not every broker has every topic, hence you will connect to all brokers
* Topic Replication Factor (should be > 1 , usually 2-3) - IF one broker is down than other can server data
	3 is a good idea because if 1 is taken down for maintenance, you still have 2 servers and can tolerate one server breakdown
	replication factor -> 2 duplicate partions and 2 duplicate topics
* Leader - at a particular time only one Broker can be the leader for a particular partition, and that partition will only receive and servr data
	All other brokers just replicate that data - ISR(In sync Replica)\

==> Producers - writes data to kafka
* They  have to specify the topic name and one broker to connect to and Kafka takes care of routing it to the appropriate broker.
* Producers can choose to receive acknowledgements 
	ACKS = 0 , producer doesnt wait for acknowledgement from broker	(possible data loss) -- high performance
	ACKS = 1 , producer waits for acknowledgement from leader (limited data loss) --
	ACKS = all , producer waits for acknowledgements from all the brokers (no data loss) -- low performance
* Producer can choose to send a key --> all the messages with that key will go into the same partition

==> Consumers - consume data from kafka
* They have to specify the topic name and one broker to connect to, kafka will pull the data from the right broker
* Data is read in order for each partition
* Consumer can consume data parallely from different partitions

==> Consumer groups - partitions are divided between consumers to read from 
	- Group of consumers read from same topic but from exclusive partitions
	- If there are more consumers than partitions then some will be inactive
	Consumer offsets - Kafka stores offset at which a consumer was reading and commits that into _consumer_offsets
					 - when a consumer reads data from kafka and processes, it commits offset
					 - When a consumer dies, it can start from where it left off
==> ZooKeeper - manages brokers, keeps a list of them
			  - helps in performing leader election for partitions
			  - Kafka can't work without zookeeper
			  - ZooKeeper has a concept of Leader and Follower
			  - ZooKeeper works in a cluster of 3,5,7 etc odd qorum of servers
			  - Kafka cluster talks to only leader, zookeeper does elections
Consumer Offset commits - 
* At most once -
	Offsets are committed as soon as the messages are received, if processing goes wrong that message in not available again
* At least once - 
	Offsets are committed after the messages are processed, or else that message is read again - can result in processing of duplicate messages, processing should be idempotent
	Idempotent - If done more than once shouln't affect the end result , Inserting is not idempotent as 2 inserts would create 2 rows, update is idempotent.
* Exactly once - 

* Run this command to run kafka from a docker image - 
[narayana@mindlinux ~]$ sudo docker run --rm -it \
> -p 2181:2181 -p 3030:3030 -p 8081:8081 \
> -p 8082:8082 -p 8083:8083 -p 9092:9092 \
> -e ADV_HOST=127.0.0.1 \
> landoop/fast-data-dev 

* Start a terminal in kafka using - 
	>> docker run --rm -it --net=host landoop/fast-data-dev bash ----> here you can run kafka commands
===> Kafka commands -
1. >> kafka-topics --zookeeper <string: URL of zookeeper> --create 
		--> root@fast-data-dev / $ kafka-topics --zookeeper 127.0.0.1:2181 --create --topic first-topic --partitions 3 --replication-factor 1
    Note - replication factor can't be more than the number of brokers.
	--describe option to tell about the topic
2. Publish data to topic - 
	root@fast-data-dev / $ kafka-console-producer  --broker-list 127.0.0.1:9092 --topic second-topic  (refer UI for brocker list)
3. Consume data from topic - 
	--broker-list(in case of producer) and --bootstrap-servers(in case of consumer) are the same 
[Note - Only time you have to interact with Zookeeper is when creating/modifying/deleting topic , No need for kafka interaction while consuming/producing data]
* When using consumer group, offset is committed , subsequent --from-beginning won't list the messages from the beginning of topic
* Done - Kafka topic and Producer -
* Samples git URL - https://github.com/apache/kafka/tree/trunk/examples/src/main/java/kafka/examples
========================================================================================================================================
Streams API -
--------------------------------------------------------
* Easy data processing and transformation library , Fraud detection , monitoring + Alerting - just a plain app and need not run in clusters - scaling is
	just by adding more Java processes - no reconfiguration
* Processes one record at a time - no batching
* kafka stream apps - from Kafka to Kafka. Write data from source topic to sink topic 
* Exactly-once capabilities.
* Streams app always run, each update is printed to kafka - batch only prints the final output  - per data streaming
* Stream processing creates internal topics like changelog and repartition, 
* Stream - A sequence of immutable data records, data cannot change 
* Stream processor - Is a node in the processor topology(graph). It transforms incoming streams, record by record - may create a new stream
* Topology - A graph of nodes/processors connected by streams
* Source processor - takes data directly from a kafka topic and creates a stream out of it, doesnt transfom the data- No predecssors in topology
* Sink processor - Doesn't have children, sends the stream directly to a kafka topic
Note - Always set "application.id" in the stream app properties 
	Behind scenes this will be - consumer { group.id = application.id }  --> may cause kafka tot think that this is a new consumer app and re-processes data
	Default client.id prefix
	Prefix to internal changelog topics 
* Set default.[key|value].serde --> for serialization and De-serialization  
* Java 8 - Lambda functions 
	- Used in case of implementing interfaces with one method

Word Count - Streams App Topology
-----------------------------------------------------
Kafka streams - data in the form of <key, value>
1. Stream - create a stream from kafka topic  <null, "Kafka Kafka Streams">
2. MapValues - convert the calue into lower case - <null, "Kafka Kafka Streams"> 
3. FlatMapValues - split the value into words <null, "kafka"> <null, "kafka"> <null, "streams">
4. SelectKey - apply a key instead of a null key - ex, key = value <"kafka","kafka"> <"kafka","kafka"> <"streams","streams">
5. GroupByKey - groups by keys - (<"kafka","kafka"> <"kafka","kafka">) (<"streams","streams">)
6. Count - occurences in each group - <"kafka" , 2> <"kafka" , 1>
7. To - use to for writing data back to kafka
   KafkaStreams streams = new KafkaStreams(builder, props) --> create a streams app
* Topology - represents all the streams and processes in the streams App
* Close the streams application gracefully (to do shutdown activities) - addShutdownHook to runtime  

 KTable<String, Long> wordCounts = wordCountStream.mapValues(value -> value.toLowerCase())
				.flatMapValues(value -> Arrays.asList(value.split(" ")))
				.selectKey((oldKey, value) -> value)
				.groupByKey().count("counts");
				
* Word Count Internal topics - 2 types (used to save/restore state)
	- Repartitioning topics - In case of transforming keys of the stream , repartitioning will happen
	- Changelog - kafka will save compacted data after aggregation in these topics 
* Fat Jar - code + all external dependencies inside that Jar
* Scaling streams app , run as many instances as the number of partitions without changing the application.id - 
	it will all consume the same topic but different partitions - distributes tasks dynamically - rebalancing happens when there is a change in instances

	Kafka streams example - https://docs.confluent.io/current/installation/docker/docs/tutorials/kafka-streams-examples.html 
	
KStreams and KTables -
---------------------------------------------------
KStreams - All inserts, Infinite, unbounded data streams - continously reads messages from a topic , appends the message to stream even in case of key repetitions
Ktable - All upserts on null values , non null values are deleted automatically , updates the value in case the same key comes again

Stateless - Result of a transformation depends only on the datapoint that is processed , ex - multiply by 2  
Stateful - Result of a transformation depends on the current value and also the past(External information) - ex count of a word

MapValues - Used in case where only value needs to be affected , doesn't trigger a repartition (KStreams + KTables)
Map - Both keys and values are affected , triggers repartitioning , Only Kstreams

Filter - Takes one record and produces 0 or one record, doesn't change keys / values | filterNot - inverse of filter

FlatMapValues - takes one record , creates 0, 1 or more records. Doesn't change key - no repartition
			  - For example , ("alice", "alice is nice") ---> flatMapValues --> ("alice", "alice") ("alice", "is") ("alice", "nice")
FlatMap - Changes keys, triggers repartition

KStream branch - Create more branches based on some conditions - evaluates all conditions in order
		KStream<String, String>[] branches = stream.branch(
			(key, value) -> value > 100,
			(key, value) -> value > 10,
			(key, value) -> value > 1,
		);
		
SelectKey - Assigns a new key to the record , key changed - hence repartitioning happens - seperate this step out to know when repartitioning happens

KTable - builder.table( ) --> deletes null values, inserts only non null values 
``
GlobalKTable = builder.globalTable(key serdes, value serdes , topic-name)

Writing back to kafka - If you write a KTable back to kafka, think about creating a log compacted topic - reduces recovery time 
	stream.to(topicname with serdes)
	Kstream newStream = stream.through (topic name with serdes) --> Write to a topic and get a stream from it 

Repartition - kafka has to write data back to kafka for re-shuffling and read, incurs network cost - performance reduces 
			- Needed when using groupByKey 
			
Log - Compaction 
-----------------------------------------------------
LogCleaupPolicy : compact --> Keep only the last updated key in the log
	- For all updated keys - older value record is deleted 
	- hold only recent updated data 
	- Offsets donot change when message is deleted , it's skipped 

KStreams and KTable duality -
Stream as a table - Stream can be a changelog of a table, each record can be state change of a table 
Table as a stream - Table can be considered as a snapshot , at a time with the latest value for each key 

KTable to Kstream --> to keep a changelog(all updates , deletes, insertions etc) of all the changes to the KTable 
		table.toStream();
KStream to KTable --> 1. groupByKey chained by aggregation
					  2. write to kafka in an intermediate topic and read as a stream from that intermediate topic

					 
	
